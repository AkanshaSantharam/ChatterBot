{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport re\nimport pickle\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"id":"4Gj3qeNuln2N","execution":{"iopub.status.busy":"2022-03-03T15:28:09.946870Z","iopub.execute_input":"2022-03-03T15:28:09.947822Z","iopub.status.idle":"2022-03-03T15:28:15.989390Z","shell.execute_reply.started":"2022-03-03T15:28:09.947702Z","shell.execute_reply":"2022-03-03T15:28:15.988555Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"\"\"\"\nINPUTS -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> \n\nAttention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n\nATTENTION OUTPUT, ACTUAL OUTPUT(INPUT) -> DECODER -> FINAL OUTPUT\n\"\"\"\n\n\"\"\"\nENCODER ARCHITECTURE:-\n\nINPUTS -> EMBEDDING -> GRU\n\"\"\"\n\n\"\"\"\nATTENTION NETWORK ARCHITECTURE:-\n\nENC OUTPUTS     -> ENC LAYER     -> --------       \n                                            ------> ACTIVATION -> FINAL LAYER -> ATTENTION WEIGHTS\nTHOUGHT VECTOR  -> THOUGHT LAYER -> --------\n\n\"\"\"","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n        super(Encoder, self).__init__()\n        \n        self.batch_size = batch_size\n        self.enc_units = encoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n    \n    def call(self, inputs, hidden_state):\n        embedded_inputs = self.embedding(inputs)\n        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n        \n        return enc_outputs, thought_vector","metadata":{"id":"9Hs51W_Jln2S","execution":{"iopub.status.busy":"2022-03-03T15:28:15.991150Z","iopub.execute_input":"2022-03-03T15:28:15.991441Z","iopub.status.idle":"2022-03-03T15:28:16.001395Z","shell.execute_reply.started":"2022-03-03T15:28:15.991401Z","shell.execute_reply":"2022-03-03T15:28:16.000698Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        \n        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, enc_outputs, thought_vector):\n        thought_matrix = tf.expand_dims(thought_vector, 1)\n        \n        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n        \n        attention_output = attention_weights * enc_outputs # Shape (batch_size, num_outputs, output_size)\n        attention_output = tf.reduce_sum(attention_output, axis=1) # New shape (batch_size, output_size)\n        \n        return attention_output, attention_weights","metadata":{"id":"E7fbSdDAln2T","execution":{"iopub.status.busy":"2022-03-03T15:28:16.002740Z","iopub.execute_input":"2022-03-03T15:28:16.003100Z","iopub.status.idle":"2022-03-03T15:28:16.020392Z","shell.execute_reply.started":"2022-03-03T15:28:16.003057Z","shell.execute_reply":"2022-03-03T15:28:16.019855Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n        \n        self.batch_size = batch_size\n        self.dec_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n        self.attention = Attention(self.dec_units)\n        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, inputs, enc_outputs, thought_vector):\n        attention_output, attention_weights = self.attention(enc_outputs, thought_vector) #Shape of attention output (batch_size, size_of_embedding)\n        \n        embedded_inputs = self.embedding(inputs) #Shape (batch_size, num_words, size_of_embedding)\n        attention_output = tf.expand_dims(attention_output, 1) #Shape of attention output (batch_size, 1, size_of_embedding)\n        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n        \n        decoder_outputs, hidden_state = self.gru(concat_inputs) #Shape (batch_size, 1, size_of_embedding)\n        decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2])) #Shape (batch_size, size_of_embedding)\n        \n        final_outputs = self.word_output(decoder_outputs)\n        \n        return final_outputs, hidden_state, attention_weights","metadata":{"id":"MEs2iQroln2U","execution":{"iopub.status.busy":"2022-03-03T15:28:16.021919Z","iopub.execute_input":"2022-03-03T15:28:16.022361Z","iopub.status.idle":"2022-03-03T15:28:16.038313Z","shell.execute_reply.started":"2022-03-03T15:28:16.022334Z","shell.execute_reply":"2022-03-03T15:28:16.037274Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        self.optimizer = tf.keras.optimizers.Adam()\n        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        \n    def loss_function(self, y_real, y_pred):\n        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n        base_loss = self.base_loss_function(y_real, y_pred)\n        \n        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n        final_loss = mask * base_loss\n        \n        return tf.reduce_mean(final_loss)\n    \n    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n        loss = 0\n        \n        with tf.GradientTape() as tape:\n            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n            dec_hidden = thought_vector\n            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n            \n            for index in range(1, label_data.shape[1]):\n                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n                \n                dec_input = tf.expand_dims(label_data[:, index], 1)\n                loss = loss + self.loss_function(label_data[:, index], outputs)\n        \n        word_loss = loss / int(label_data.shape[1])\n        \n        variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        self.optimizer.apply_gradients(zip(gradients, variables))\n        \n        return word_loss","metadata":{"id":"8HRY3Weoln2V","execution":{"iopub.status.busy":"2022-03-03T15:28:16.040777Z","iopub.execute_input":"2022-03-03T15:28:16.041476Z","iopub.status.idle":"2022-03-03T15:28:16.056805Z","shell.execute_reply.started":"2022-03-03T15:28:16.041432Z","shell.execute_reply":"2022-03-03T15:28:16.055779Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"EXAMPLE:\n[he she it name this that these those their you]\n\ny_real = [0, 0, 0, 0, '1', 0, 0, 0, 0, 0]\n\nmath.equal() = [True, True, True, True, False, True, True, True, True, True]\nlogical not = \nmask = [1, 1, 1, 1, '0', 1, 1, 1, 1, 1]\n\n[0.001, 0.001, 0.001, 0.001, '0.9', 0.001, 0.001, 0.001, 0.001, 0.002]\n\n\n[1, 1, 1, 1, '0', 1, 1, 1, 1, 1] * [0.001, 0.001, 0.001, 0.001, '-0.1', 0.001, 0.001, 0.001, 0.001, 0.002]\n\nfinal_loss = [0.001, 0.001, 0.001, 0.001, 0, 0.001, 0.001, 0.001, 0.001, 0.002]\n\nreturn 0.0013\n","metadata":{}},{"cell_type":"code","source":"class Data_Preprocessing:\n    def __init__(self):\n        self.temp = None\n    \n    def get_data(self, path):\n        file = open(path, 'r').read()\n        lists = [f.split('\\t') for f in file.split('\\n')]\n        \n        questions = [x[0] for x in lists]\n        answers = [x[1] for x in lists]\n        \n        return questions, answers\n    \n    def process_sentence(self, line):\n        line = line.lower().strip()\n        \n        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n        line = re.sub(r'[\" \"]+', \" \", line)\n        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n        line = line.strip()\n        \n        line = '<start> ' + line + ' <end>'\n        return line\n    \n    def word_to_vec(self, inputs):\n        tokenizer = Tokenizer(filters='')\n        tokenizer.fit_on_texts(inputs)\n        \n        vectors = tokenizer.texts_to_sequences(inputs)\n        vectors = pad_sequences(vectors, padding='post')\n        \n        return vectors, tokenizer","metadata":{"id":"xyWEx8sSln2W","execution":{"iopub.status.busy":"2022-03-03T15:28:16.057984Z","iopub.execute_input":"2022-03-03T15:28:16.058320Z","iopub.status.idle":"2022-03-03T15:28:16.072902Z","shell.execute_reply.started":"2022-03-03T15:28:16.058291Z","shell.execute_reply":"2022-03-03T15:28:16.072095Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = Data_Preprocessing()\n\nquestions, answers = data.get_data('../input/chatbots/chatbot.txt')","metadata":{"id":"uQupNbmXln2X","execution":{"iopub.status.busy":"2022-03-03T15:28:16.074168Z","iopub.execute_input":"2022-03-03T15:28:16.075049Z","iopub.status.idle":"2022-03-03T15:28:16.101950Z","shell.execute_reply.started":"2022-03-03T15:28:16.075009Z","shell.execute_reply":"2022-03-03T15:28:16.101337Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"questions = [data.process_sentence(str(sentence)) for sentence in questions]\nanswers = [data.process_sentence(str(sentence)) for sentence in answers]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:28:16.103444Z","iopub.execute_input":"2022-03-03T15:28:16.103752Z","iopub.status.idle":"2022-03-03T15:28:16.285018Z","shell.execute_reply.started":"2022-03-03T15:28:16.103712Z","shell.execute_reply":"2022-03-03T15:28:16.284311Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_vectors, train_tokenizer = data.word_to_vec(questions)\nlabel_vectors, label_tokenizer = data.word_to_vec(answers)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:28:16.286321Z","iopub.execute_input":"2022-03-03T15:28:16.286843Z","iopub.status.idle":"2022-03-03T15:28:16.557370Z","shell.execute_reply.started":"2022-03-03T15:28:16.286781Z","shell.execute_reply":"2022-03-03T15:28:16.556470Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vocab_train = len(train_tokenizer.word_index) + 1\nvocab_label = len(label_tokenizer.word_index) + 1","metadata":{"id":"LiM_lyK8ln2Y","execution":{"iopub.status.busy":"2022-03-03T15:28:16.559599Z","iopub.execute_input":"2022-03-03T15:28:16.559838Z","iopub.status.idle":"2022-03-03T15:28:16.564695Z","shell.execute_reply.started":"2022-03-03T15:28:16.559811Z","shell.execute_reply":"2022-03-03T15:28:16.563580Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"max_length_train = train_vectors.shape[1]\nmax_length_label = label_vectors.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:28:16.565936Z","iopub.execute_input":"2022-03-03T15:28:16.566268Z","iopub.status.idle":"2022-03-03T15:28:16.576011Z","shell.execute_reply.started":"2022-03-03T15:28:16.566237Z","shell.execute_reply":"2022-03-03T15:28:16.575186Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nbuffer_size = train_vectors.shape[0]\nembedding_dim = 256\nsteps_per_epoch = buffer_size//batch_size\nunits = 1024","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:28:16.577417Z","iopub.execute_input":"2022-03-03T15:28:16.577800Z","iopub.status.idle":"2022-03-03T15:28:16.587632Z","shell.execute_reply.started":"2022-03-03T15:28:16.577766Z","shell.execute_reply":"2022-03-03T15:28:16.586563Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\ndataset = dataset.shuffle(buffer_size)\ndataset = dataset.batch(batch_size, drop_remainder=True)","metadata":{"id":"fvpb6K1sln2Y","execution":{"iopub.status.busy":"2022-03-03T15:28:16.588881Z","iopub.execute_input":"2022-03-03T15:28:16.589121Z","iopub.status.idle":"2022-03-03T15:28:16.650234Z","shell.execute_reply.started":"2022-03-03T15:28:16.589091Z","shell.execute_reply":"2022-03-03T15:28:16.649605Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\ndecoder = Decoder(vocab_label, embedding_dim, units, batch_size)\ntrainer = Train()","metadata":{"id":"jtjd9Nr7ln2Z","execution":{"iopub.status.busy":"2022-03-03T15:28:16.651740Z","iopub.execute_input":"2022-03-03T15:28:16.652189Z","iopub.status.idle":"2022-03-03T15:28:16.698125Z","shell.execute_reply.started":"2022-03-03T15:28:16.652147Z","shell.execute_reply":"2022-03-03T15:28:16.697387Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\n\nfor epoch in range(1, EPOCHS + 1):\n    enc_hidden = tf.zeros((batch_size, units))\n    total_loss = 0\n    \n    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n        total_loss = total_loss + batch_loss\n        \n    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")","metadata":{"id":"2dD7b5ffln2Z","execution":{"iopub.status.busy":"2022-03-03T15:28:16.699356Z","iopub.execute_input":"2022-03-03T15:28:16.699677Z","iopub.status.idle":"2022-03-03T17:35:50.797619Z","shell.execute_reply.started":"2022-03-03T15:28:16.699638Z","shell.execute_reply":"2022-03-03T17:35:50.796433Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class Chatbot:\n    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n        self.train_tokenizer = train_tokenizer\n        self.label_tokenizer = label_tokenizer\n        self.encoder = encoder\n        self.decoder = decoder\n        self.units = units\n        self.data = Data_Preprocessing()\n        self.maxlen = max_length_train\n    \n    def clean_answer(self, answer):\n        answer = answer[:-1]\n        answer = ' '.join(answer)\n        return answer\n    \n    def predict(self, sentence):\n        sentence = self.data.process_sentence(sentence)\n        \n        sentence_mat = []\n        for word in sentence.split(\" \"):\n            try:\n                sentence_mat.append(self.train_tokenizer.word_index[word])\n            except:\n                return \"Could not understand that, can you re-phrase?\"\n        \n        sentence_mat = pad_sequences([sentence_mat], maxlen=self.maxlen, padding='post')\n        sentence_mat = tf.convert_to_tensor(sentence_mat)\n        \n        enc_hidden = [tf.zeros((1, self.units))]\n        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n        \n        dec_hidden = thought_vector\n        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n        \n        answer = []\n        for i in range(1, self.maxlen):\n            pred, dec_hidden, _ = decoder(dec_input, encoder_outputs, dec_hidden)\n            \n            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n            answer.append(word)\n            \n            if word == '<end>':\n                return self.clean_answer(answer)\n            \n            dec_input = tf.expand_dims([np.argmax(pred[0])], 0)\n        \n        return self.clean_answer(answer)","metadata":{"id":"EKeF7WeLln2a","execution":{"iopub.status.busy":"2022-03-03T17:35:50.798864Z","iopub.execute_input":"2022-03-03T17:35:50.799413Z","iopub.status.idle":"2022-03-03T17:35:50.811026Z","shell.execute_reply.started":"2022-03-03T17:35:50.799381Z","shell.execute_reply":"2022-03-03T17:35:50.810460Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)","metadata":{"id":"kmOW2zdYln2b","execution":{"iopub.status.busy":"2022-03-03T17:35:50.811988Z","iopub.execute_input":"2022-03-03T17:35:50.812336Z","iopub.status.idle":"2022-03-03T17:35:50.828776Z","shell.execute_reply.started":"2022-03-03T17:35:50.812307Z","shell.execute_reply":"2022-03-03T17:35:50.827868Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"question = ''\nwhile True:\n    question = str(input('You:'))\n    if question == 'quit' or question == 'Quit':\n        break\n        \n    answer = bot.predict(question)\n    print(f'Bot: {answer}')","metadata":{"id":"n8upf7E_ln2b","execution":{"iopub.status.busy":"2022-03-03T17:35:50.830363Z","iopub.execute_input":"2022-03-03T17:35:50.830870Z","iopub.status.idle":"2022-03-03T17:38:50.162392Z","shell.execute_reply.started":"2022-03-03T17:35:50.830797Z","shell.execute_reply":"2022-03-03T17:38:50.161501Z"},"trusted":true},"execution_count":18,"outputs":[]}]}